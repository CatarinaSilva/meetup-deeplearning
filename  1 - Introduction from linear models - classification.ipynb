{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 - Introduction from linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is strongly based on a programming assignment of coursera's \"Introduction to Deep Learning\" (see here https://www.coursera.org/learn/intro-to-deep-learning), although the exercises and purpose is not the same. We use a similar example generated by us, and a similar feature choice. We additionally borrow some of the visualizations shown there, since they are very powerfull for understanding the functioning of these methods. However, the purpose of this notebook is for you to implement different functions. If you have a solid foundation of the basics and want to continue learning about deep learning, I recomned you do that one or a number of other courses available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification problem and linear models\n",
    "\n",
    "In this notebook we will review the concept of classification problem, and use a slightly more complicated problem than in the previous notebook to show you the usefullness of training and other techniques used such as mini-batching.\n",
    "\n",
    "The purpose will be to try to come up with a linear model that classifies sample into one of two categories. Start by loading the data and visualizing it's classes, depicted by different colors. This time, we'll assume we don't know the model beforehand. As you can see the data is not linearly separable, which will mean we need either non linear features (like the ones in the polynomial model) or a non-linear model.\n",
    "\n",
    "\n",
    "NOTE: The seed is what will allows us reproducibility. Do not change it so that you can compare the outputs during this event. Change it afterwards to see that the method works even with different random samples for the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source_data.npy', 'rb') as fin:\n",
    "    X = np.load(fin)\n",
    "    \n",
    "with open('label_data.npy', 'rb') as fin:\n",
    "    y = np.load(fin)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "The problem we present is a classification problem with two classes. One common approach is to model the problem as of discovering the probability of a certain example belonging to the default class, this is, if you have class y = 0 and class y = 1, we can pick as default 1 and model:\n",
    "\n",
    "$$P(X) = P(Y=1|X)$$\n",
    "\n",
    "We then define some linear model computed from a number of features to try to model our problem:\n",
    "\n",
    "$$ a(x) = w_0 + w_1 x_0 + w_2 x_1 + ... + w_f x_{f-1} $$ \n",
    "\n",
    "However, we need to find a way to map the output into probabilities, and for that we will use the logistic function $\\sigma(x)$:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Implement this function below and verify it converts values to the probability range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Implements logistic function\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.linspace(-10, 10, 1000)\n",
    "test_sig = sigmoid(test_x)\n",
    "\n",
    "plt.plot(test_x, test_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see that most values are converted to an output that can be interpreted as a probability. Putting everything together the task is that of finding weights $w_0 ... w_f$ that optimize our model $a(x)$ where the output $y$ of this model is then passed through the logistic function to obtain the probability of $x$ belonging to the default class. We call this task a logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "So, once again, how do we know how well our model adapts to the data?\n",
    "\n",
    "For classification problems, we do not use the mean squared error. In logistic regression we rely on cross-entropy minimization, where the loss is defined as \n",
    "\n",
    "$$ l(x_i, y_i, w) = - \\left[ {y_i \\cdot log P(y_i = 1 \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i = 1\\, | \\, x_i,w))}\\right] $$\n",
    "\n",
    "Cross entropy is generally taken as a measure of distance between two probability distributions, so in this particular case, we use it as a way to measure distance between our model and the true distribution.\n",
    "\n",
    "For many samples, it's just an average over the measure in each sample:\n",
    "\n",
    "$$ L(X, \\vec{y}, w) =  {1 \\over \\ell} \\sum_{i=1}^\\ell l(x_i, y_i, w) $$\n",
    "\n",
    "Implement the loss function for a set of samples below (don't forget that the probability can be given by the sigmoid of the output of our model $a(x) = wX$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between probability obtained from model a(x) = wX \n",
    "    and true distribution given by labels y. \n",
    "    \n",
    "    X is our feature matrix, y the labels for each sample in matrix X and w the weight vector\n",
    "    that represents the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function in the dummy examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_w_1 = np.array([0.0, 0.0])\n",
    "assert compute_cross_entropy_loss(X, y, dummy_w_1) == 0.6931471805599452\n",
    "\n",
    "dummy_w_2 = np.array([0.5, 0.5])\n",
    "assert compute_cross_entropy_loss(X, y, dummy_w_2) == 0.9879046219329977\n",
    "\n",
    "dummy_w_3 = np.array([-0.5, 0.5])\n",
    "assert compute_cross_entropy_loss(X, y, dummy_w_3) == 1.0654760708249085\n",
    "\n",
    "dummy_w_4 = np.array([0.5, -0.5])\n",
    "assert compute_cross_entropy_loss(X, y, dummy_w_4) == 0.7365381401333866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the problem\n",
    "\n",
    "You implemented the basic functions representing a logistic regression, but at this point we haven't defined our model and the features we're going to use.\n",
    "\n",
    "Since the purpose of this notebook is not feature engineering, we're going to give you the features. Use the following features:\n",
    "\n",
    "* $x_0 = x$\n",
    "* $x_1 = y$\n",
    "* $x_2 = x^2$\n",
    "* $x_3 = y^2$\n",
    "* $x_4 = xy $\n",
    "\n",
    "With these non linear features, we are allowing our linear model to perform the non linear separation required. We suggest that later on you try different features to see how the model behaves.\n",
    "\n",
    "Implement the expansion function below to generate our feature matrix $X$ from the two initial components in the sample.\n",
    "\n",
    "NOTE: don't forget to add a 1-vector component to represent the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_xy(samples):\n",
    "    \"\"\"\n",
    "        Function to expand a matrix of samples (x, y) into non linear features\n",
    "        \n",
    "        samples - matrix of size [n_samples, 2] with components x and y for each sample\n",
    "        \n",
    "        The output should be a matrix of size N x 6, with all features plus the \n",
    "        component representing the bias\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function in the dummy examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = np.array(\n",
    "    [\n",
    "        [0.0, 0.0], \n",
    "        [0.0, 1.0], \n",
    "        [1.0, 0.0], \n",
    "        [1.0, 1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_expanded_x = np.array(\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0], \n",
    "        [0.0, 1.0, 0.0, 1.0, 0.0, 1.0], \n",
    "        [1.0, 0.0, 1.0, 0.0, 0.0, 1.0], \n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert expand_xy(dummy_x).shape == (4, 6)\n",
    "assert (expand_xy(dummy_x) == dummy_expanded_x).all()\n",
    "\n",
    "dummy_x_2 = np.array(\n",
    "    [\n",
    "        [1.0, 2.0], \n",
    "        [2.0, 2.0], \n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_expanded_x_2 = np.array(\n",
    "    [\n",
    "        [1.0, 2.0, 1.0, 4.0, 2.0, 1.0], \n",
    "        [2.0, 2.0, 4.0, 4.0, 4.0, 1.0], \n",
    "    ]\n",
    ")\n",
    "\n",
    "assert expand_xy(dummy_x_2).shape == (2, 6)\n",
    "assert (expand_xy(dummy_x_2) == dummy_expanded_x_2).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "We're only missing one last thing to be able to apply the same techniques to this problem. In the same way as the polynomial regression, we now want to use the SGD to find the best weights to our model. We first need to define the gradient function as we did for the regression. The iterative process is the same:\n",
    "\n",
    "$$ w_{i+1} = w_i - \\eta \\Delta_w L$$ \n",
    "\n",
    "where $\\Delta_w L$ is the gradient of your loss function. Our loss function as changed, however, and so the derivative is now represented by:\n",
    "\n",
    "$$ \\Delta_w = \\frac{1}{l}X^T(\\sigma(Xw) - y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_cross_entropy(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    Keep in mind that our loss is averaged over all samples (rows) in X.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function in the dummy examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x_1 = np.array(\n",
    "    [\n",
    "        [0.0, 0.0], \n",
    "        [0.0, 1.0], \n",
    "        [1.0, 0.0], \n",
    "        [1.0, 1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_expanded_x_1 = expand_xy(dummy_x_1)\n",
    "dummy_y_1 = np.array([1.0, 0.0, 1.0, 0.0])\n",
    "dummy_weights_1 = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "dummy_grads_1 = compute_gradient_cross_entropy(dummy_expanded_x_1, dummy_y_1, dummy_weights_1) \n",
    "\n",
    "assert (dummy_grads_1 == np.array([0.0, 0.25, 0.0, 0.25, 0.125, 0.0])).all()\n",
    "\n",
    "\n",
    "dummy_x_2 = np.array(\n",
    "    [\n",
    "        [1.0, 2.0], \n",
    "        [2.0, 2.0], \n",
    "    ]\n",
    ")\n",
    "\n",
    "dummy_expanded_x_2 = expand_xy(dummy_x_2)\n",
    "dummy_y_2 = np.array([0.0, 1.0])\n",
    "dummy_weights_2 = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "dummy_grads_2 = compute_gradient_cross_entropy(dummy_expanded_x_2, dummy_y_2, dummy_weights_2) \n",
    "\n",
    "assert (dummy_grads_2 == np.array([-0.25, 0.0, -0.75, 0.0, -0.5, 0.0])).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "We now have all required pieces to run SGD. We will apply SGD first by running each step in all samples, as we did for the regression. \n",
    "\n",
    "We'll first define a visualization function to see how well our models fit the data. This will plot the data together with your model classification. Test it out in some random weights below to get familiar with the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "def visualize(X, y, w, history):\n",
    "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
    "    Z = sigmoid(np.matmul(expand_xy(np.c_[xx.ravel(), yy.ravel()]), w))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = expand_xy(X)\n",
    "dummy_weights = np.linspace(-1, 1, 6)\n",
    "visualize(X_features, y, dummy_weights, [0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the left should represent the data and the model classification in background colours. The plot on the right should plot the error history.\n",
    "\n",
    "We can now apply the SGD to this problem. In the cycle below, we use the gradient you implemented to converge to a set of weights. We'll use the visualization function from before to see the variation of the errors, our samples and the fit of our model at each 5 steps.\n",
    "\n",
    "Run the cycle below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please use np.random.seed(42), eta=0.1, n_iter=100 and batch_size=4 for deterministic results\n",
    "\n",
    "np.random.seed(42)\n",
    "w = 0.0001 * (0.5 - np.random.rand(6))  # initial weights\n",
    "eta = 0.1 # learning rate\n",
    "\n",
    "n_iter = 100\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(n_iter):\n",
    "    loss[i] = compute_cross_entropy_loss(X_features, y, w)\n",
    "    if i % 5 == 0:\n",
    "        visualize(X_features, y, w, loss)\n",
    "\n",
    "    grad = compute_grad(X_features, y, w)\n",
    "    w = w - eta*grad\n",
    "    w = w.flatten()\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some variations of the gradient descent that are important for you to know:\n",
    "\n",
    "#### Stochastic gradient descent (SGD)\n",
    "\n",
    "SGD uses each example and updates the weights separately. Because of this, it is often called an online machine learning algorithm. The frequent updating of the weights should update the rate at which the model improves, and it is quite simple to implement and understand. For some problems, the model might converge faster, however, as a downside, depending on the data, the process can be extremely noisy and get the model stuck in local minima more easily. In most cases, this method tends to have a higher variance over training epochs. Additionally, frequent updates might be computationally expensive, which is a big downside if your dataset is very big.\n",
    "\n",
    "#### Batch gradient descent \n",
    "\n",
    "Batch gradient descent is quite similar to what we've been running. Even for large datasets, it will iterate over the error for each example, storing the loss and gradients, but will only update the model when all examples have been seen. We call each pass through all of the data one training epoch. Because it has fewer updates, the implementation might be less computationally expensive and more stable, this is, having a lower variance between training epochs. However, the convergence might be slowed down, in particular for very large datasets. Finally, accumulating all of the prediction errors across all examples might increase complexity.\n",
    "\n",
    "#### Mini-Batch gradient descent \n",
    "\n",
    "Mini batch gradient descent is an intermediate solution, taking advantage of the speed gains of doing faster updates and the stability gains of using more examples. At each iterations, it gets a batch of examples from the training set, what we call the mini batch, and computes the loss and gradients over these examples. It then updates the model accordingly, and continues to the next iterations. This is the most common implementation use, but it requires an extra hyperparameter to be defined, the batch size.\n",
    "\n",
    "\n",
    "Implement the mini batch variation below. Run it and look at the error variation, which, although not as smooth as the previous cycle, converges quite well to similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please use np.random.seed(42), eta=0.1, n_iter=100 and batch_size=4 for deterministic results\n",
    "\n",
    "np.random.seed(42)\n",
    "w = np.array([0, 0, 0, 0, 0, 1])\n",
    "eta= 0.1 # learning rate\n",
    "batch_size = 4\n",
    "\n",
    "n_iter = 100\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(n_iter):\n",
    "    loss[i] = compute_cross_entropy_loss(X_features, y, w)\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        visualize(X_features, y, w, loss)\n",
    "\n",
    "    # YOUR CODE HERE - CHANGE PREVIOUS IMPLEMENTATION TO MINI-BATCHING\n",
    "    raise Exception('Not implemented')\n",
    "    \n",
    "    grad = compute_grad(X_features, y, w)\n",
    "    w = w - eta*grad\n",
    "    w = w.flatten()\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this problem, we introduced logistic regression, and the logitic function, as well as the cross-entropy loss which will be used later on. We also introduced you to the several variations of gradient descent, including the mini batch gradient descent, which is the most commonly used in deep learning algorithms.\n",
    "\n",
    "As in the previous notebook, try playing around with some parameters and see its impact. Additionally, try implementing also the simples SGD algorithm to see how it converges.\n",
    "\n",
    "In the next notebook, we will already implement a simple neural network, making use of some of the concepts we've seen so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

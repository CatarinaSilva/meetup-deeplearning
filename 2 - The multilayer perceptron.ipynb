{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - The multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation - sigmoid\n",
    "\n",
    "We'll first implement the required methods to represent it. As you recall, the sigmoid is defined by the following expression:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Which corresponds to a function of S-shape, that cuts values to an output between 0 and 1 \n",
    "\n",
    "<img src='sigmoid.png' />\n",
    "\n",
    "It's derivative is also quite simple, which will be usefull when computing the required gradients:\n",
    "\n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z))$$\n",
    "\n",
    "Implement these in the following class, to be used as the activation of our MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "        Implementation of the sigmoid activation function\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def forward(cls, z):\n",
    "        \"\"\"\n",
    "        Implementation of sigmoid\n",
    "        \"\"\"\n",
    "        raise Exception('Not Implemented')\n",
    "    \n",
    "    @classmethod\n",
    "    def backward(cls, z):\n",
    "        \"\"\"\n",
    "        Implementation of derivative of sigmoid\n",
    "        \"\"\"\n",
    "        raise Exception('Not Implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out in the dummy examples below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Sigmoid.forward(0) == 0.5\n",
    "assert Sigmoid.backward(0) == 0.25\n",
    "\n",
    "assert np.allclose(Sigmoid.forward(np.array([0, 1])), np.array([0.5, 0.73105858]))\n",
    "assert np.allclose(Sigmoid.backward(np.array([0, 1])), np.array([0.25, 0.19661193]))\n",
    "\n",
    "assert np.allclose(Sigmoid.forward(np.array([1, 0])), np.array([0.73105858, 0.5]))\n",
    "assert np.allclose(Sigmoid.backward(np.array([1, 0])), np.array([0.19661193, 0.25]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move forward to the actual MLP. We already provide you with a class with the necessary struture, so that you can focus only on the required steps - feedforward and backpropagation. However, you should go over and understand the other methods presented, so that you can implement further on top of the structure.\n",
    "\n",
    "## Feedforward \n",
    "\n",
    "You will start by implement the feedforward pass. Here, you want to compute the linear quantity of each node and its output. Use the expressions below to guide you:\n",
    "\n",
    "$$\\vec{z}^{lin} = \\vec{x}^TW$$\n",
    "\n",
    "$$\\vec{z} = g(\\vec{z}^{lin})$$\n",
    "\n",
    "\n",
    "Implement the forward pass below for each layer, in the correct method (`layer_feedforward`). Notice that in the `feedforward` function we make use of your implementation and already make use of the auxiliar function `extend_bias` to add a collumn of ones to your layer's inputs whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleOutputMLP:\n",
    "    \"\"\" Simple implementation of a Multi-layer perceptron\"\"\"\n",
    "    \n",
    "    num_layers = 1\n",
    "    layer_sizes = []\n",
    "    weights = None\n",
    "    activation = None\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    def __init__(self, layers, activation=Sigmoid):\n",
    "        self.num_layers = len(layers) + 1\n",
    "        self.layer_sizes = layers + [1]\n",
    "        self.activation = activation\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights randomly from uniform distributed shifted to represent [-0.5, 0.5]\n",
    "        There are much better initialization methods, but we'll keep this for the sake of simplicity\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            size_layer = self.layer_sizes[i]\n",
    "            size_next_layer = self.layer_sizes[i+1]\n",
    "            layer_weights = (np.random.rand(size_layer + 1, size_next_layer) - 0.5)\n",
    "            weights.append(layer_weights)\n",
    "            \n",
    "        self.weights = weights\n",
    "\n",
    "    def extend_bias(self, X):\n",
    "        \"\"\"\n",
    "        Auxiliar function that adds a collumn of ones for the bias computation\n",
    "        \"\"\"\n",
    "        samples = X.shape[0]\n",
    "        return np.concatenate((X, np.ones([samples ,1])), axis=1)\n",
    "        \n",
    "    def layer_feedforward(self, X, w):\n",
    "        \"\"\"\n",
    "        Implementation of the Feedforward for one layer\n",
    "        \n",
    "        Should receive:\n",
    "        \n",
    "        X [sample_size, input_layer_size + 1] - the input of the layer (already with line for bias)  \n",
    "        w [input_layer_size + 1, output_layer_size] - the weights of the layer  \n",
    "        \n",
    "        Should return (Z, Z_lin)\n",
    "        Z [output_layer_size, 1] - the outputs the layer \n",
    "        Z_lin [output_layer_size, 1] - the linear combination of inputs for the layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise Exception('Not Implemented')\n",
    "        \n",
    "    def feedforward(self, X):\n",
    "        \"\"\"\n",
    "        Implementation of the Feedforward Pass\n",
    "        \n",
    "        Should receive the input X and return Z - the outputs for each layer - and Z_lin - the linear\n",
    "        combination of inputs for the layer\n",
    "        \"\"\"\n",
    "                \n",
    "        # Use these arrays to keep the outputs (before and after activation) of each layer\n",
    "        Z = [None] * self.num_layers\n",
    "        Z_lin = [None] * self.num_layers\n",
    "        \n",
    "        layer_output = X\n",
    "        for ix in range(self.num_layers-1):\n",
    "            layer_weights = self.weights[ix]\n",
    "            layer_input = self.extend_bias(layer_output)\n",
    "            z, zlin = self.layer_feedforward(layer_input, layer_weights)\n",
    "            Z[ix] = layer_input\n",
    "            Z_lin[ix + 1] = zlin\n",
    "            layer_output = z\n",
    "        \n",
    "        Z[-1] = layer_output\n",
    "        return Z, Z_lin\n",
    "    \n",
    "    def output_delta_cross_entropy(self, p, y):\n",
    "        \"\"\"\n",
    "            Implementation of the loss function gradient w.r.t the linear combination of the output \n",
    "            layer\n",
    "            \n",
    "            Should receive:\n",
    "        \n",
    "            p [sample_size, output_size = 1] - the output, a.k.a, the prediction of the model \n",
    "            y [sample_size, output_size = 1] - the expected label  \n",
    "\n",
    "            Should return \n",
    "            delta_lin [sample_size, output_size = 1] - the derivative w.r.t the linear combination of inputs for the layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise Exception('Not Implemented')\n",
    "    \n",
    "    def hidden_backprop_delta(self, z_lin, w, d_next):\n",
    "        \"\"\"\n",
    "            Implementation of the Backprop delta computation for one layer\n",
    "            \n",
    "            Should receive:\n",
    "            \n",
    "            Z_lin [sample_size, layer_size] - the linear combination of the layer\n",
    "            d_next [sample_size, next_layer_size] - the deltas of the current layer for each sample\n",
    "            w_next [layer_size, next_layer_size] - the weights of the layer without bias term\n",
    "\n",
    "            Should return \n",
    "            delta [sample_size, layer_size] - deltas for the current layer\n",
    " \n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        raise Exception('Not Implemented')\n",
    "\n",
    "    def backprop_gradients(self, X, d):\n",
    "        \"\"\"\n",
    "            Implementation of the Backprop gradient computation for one layer\n",
    "            \n",
    "            Should receive:\n",
    "            \n",
    "            X [sample_size, prev_layer_size + 1] - the input of the layer (already with line for bias)  \n",
    "            d [sample_size, layer_size] - the deltas of the current layer for each sample\n",
    "\n",
    "            Should return \n",
    "            gradients [prev_layer_size + 1, layer_size] - gradients for all the layers' weights (including bias term)\n",
    " \n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise Exception('Not Implemented')\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        \"\"\"\n",
    "        Implementation of the Backpropagation Pass\n",
    "        \n",
    "        Should receive the input examples X and labels Y and return gradients for weights update\n",
    "        Makes use of the previous feedforward function to get the z and z_lin in each layer\n",
    "        \"\"\"\n",
    "\n",
    "        # Feedforward\n",
    "        Z, Z_lin = self.feedforward(X)\n",
    "\n",
    "        # Deltas are the derivatives of the loss w.r.t the nodes\n",
    "        # Gradients are the derivatives of the loss w.r.t the weights for update\n",
    "        deltas = [None] * self.num_layers\n",
    "        gradients = [None] * (self.num_layers - 1)\n",
    "        deltas[-1] = self.output_delta_cross_entropy(Z[-1], y)\n",
    "\n",
    "        # Compute deltas\n",
    "        for ix in range(self.num_layers-2, 0 , -1):\n",
    "            # weights without bias\n",
    "            layer_weights = self.weights[ix][:-1,:]\n",
    "            deltas[ix] = self.hidden_backprop_delta(Z_lin[ix], layer_weights, deltas[ix + 1])\n",
    "\n",
    "        # Compute gradients\n",
    "        for ix in range(self.num_layers - 1):\n",
    "            gradients[ix] = self.backprop_gradients(Z[ix], deltas[ix + 1])\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def train(self, X, y, iterations):\n",
    "        y_2d = np.reshape(y, (y.shape[0], 1))\n",
    "        for iteration in range(iterations):\n",
    "            gradients = self.backpropagation(X, y_2d)\n",
    "            for ix in range(len(self.weights)):\n",
    "                self.weights[ix] = self.weights[ix] - self.learning_rate * gradients[ix]\n",
    "                \n",
    "    def predict(self, X):\n",
    "        Z, _ = self.feedforward(X)\n",
    "        return Z[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation:\n",
    "    @classmethod\n",
    "    def forward(cls, z):\n",
    "        return 2*z\n",
    "    \n",
    "    @classmethod\n",
    "    def backward(cls, z):\n",
    "        return 2\n",
    "\n",
    "# input of size 2 , 1 hidden layer of size 2\n",
    "dummy_mlp = SingleOutputMLP([2, 2], activation=LinearActivation)\n",
    "dummy_mlp.weights = [np.ones((3, 2)), np.ones((3, 1))]\n",
    "dummy_z, dummy_z_lin = dummy_mlp.feedforward(np.array([[1.0, 1.0]])) \n",
    "\n",
    "assert np.allclose(dummy_z[0], np.array([[1., 1., 1.]])) # input layer\n",
    "assert np.allclose(dummy_z[1], np.array([[6., 6., 1.]])) # hidden layer\n",
    "assert np.allclose(dummy_z[2], np.array([[26.]])) # output layer\n",
    "\n",
    "assert dummy_z_lin[0] == None # input layer does not have linear part\n",
    "assert np.allclose(dummy_z_lin[1], np.array([[3., 3.]])) # hidden layer\n",
    "assert np.allclose(dummy_z_lin[2], np.array([[13.]])) # output layer\n",
    "\n",
    "class LinearActivation2:\n",
    "    @classmethod\n",
    "    def forward(cls, z):\n",
    "        return z/2\n",
    "    \n",
    "    @classmethod\n",
    "    def backward(cls, z):\n",
    "        return 0.5\n",
    "\n",
    "# input of size 2 , 2 hidden layer of size 2\n",
    "dummy_mlp = SingleOutputMLP([2, 2, 2], activation=LinearActivation2)\n",
    "dummy_mlp.weights = [np.ones((3, 2)), np.ones((3, 2)), np.ones((3, 1))]\n",
    "dummy_z_2, dummy_z_lin_2 = dummy_mlp.feedforward(np.array([[0.5, 0.5]])) \n",
    "\n",
    "assert np.allclose(dummy_z_2[0], np.array([[0.5, 0.5, 1.]])) # input layer\n",
    "assert np.allclose(dummy_z_2[1], np.array([[1., 1., 1.]])) # hidden layer 1\n",
    "assert np.allclose(dummy_z_2[2], np.array([[1.5, 1.5, 1.]])) # hidden layer 2\n",
    "assert np.allclose(dummy_z_2[3], np.array([[2.]])) # output layer\n",
    "\n",
    "assert dummy_z_lin[0] == None # input layer does not have linear part\n",
    "assert np.allclose(dummy_z_lin_2[1], np.array([[2., 2.]])) # hidden layer 1\n",
    "assert np.allclose(dummy_z_lin_2[2], np.array([[3., 3.]])) # hidden layer 2\n",
    "assert np.allclose(dummy_z_lin_2[3], np.array([[4.]])) # output layer\n",
    "\n",
    "\n",
    "# more than 1 sample\n",
    "dummy_z_3, dummy_z_lin_3 = dummy_mlp.feedforward(np.array([[0.5, 0.5], [1., 1.]])) \n",
    "\n",
    "# 2 samples\n",
    "assert np.allclose(dummy_z_3[0], np.array([[0.5, 0.5, 1.], [1., 1., 1.]])) # input layer\n",
    "assert np.allclose(dummy_z_3[1], np.array([[1., 1., 1.], [1.5, 1.5, 1.]])) # hidden layer 1\n",
    "assert np.allclose(dummy_z_3[2], np.array([[1.5, 1.5, 1.], [2., 2., 1.]])) # hidden layer 2\n",
    "assert np.allclose(dummy_z_3[3], np.array([[2.], [2.5]])) # output layer\n",
    "\n",
    "assert dummy_z_lin[0] == None # input layer does not have linear part\n",
    "assert np.allclose(dummy_z_lin_3[1], np.array([[2., 2.], [3., 3.]])) # hidden layer 1\n",
    "assert np.allclose(dummy_z_lin_3[2], np.array([[3., 3.], [4., 4.]])) # hidden layer 2\n",
    "assert np.allclose(dummy_z_lin_3[3], np.array([[4.], [5.]])) # output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Now we need to implement the backpropagation. Once again, we alreday provide the base structure and only require you to fill the layer's internal computations.\n",
    "\n",
    "Start by implementing the partial derivative w.r.t the linear score of the output node in `output_delta`. Since we are using the cross entropy as the loss function, your function should compute the following:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_{lin}} =  (y-p)$$\n",
    "\n",
    "We now want to propagate this quantity to other nodes. Use the following expression and implement the `hidden_backprop_delta` function\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_{lin_i}} = \\sigma(z_{lin_i})(1 - \\sigma(z_{lin_i}))\\sum_j( \\frac{\\partial L}{\\partial z_{lin_j}} w_{ij})$$\n",
    "\n",
    "Then, implement the gratdient computation in `backprop_gradients` for every layer with the simple equation:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial z_{lin_j}} x_{i}$$\n",
    "\n",
    "\n",
    "Test each of the implementations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output delta tests\n",
    "dummy_mlp = SingleOutputMLP([2, 2], activation=LinearActivation)\n",
    "delta_function = dummy_mlp.output_delta_cross_entropy\n",
    "\n",
    "assert delta_function(1.0, 0.0) == 1.0\n",
    "assert np.allclose(delta_function(np.array([1.0, 1.0]), np.array([0., 0.])), np.array([1.0, 1.0]))\n",
    "assert np.allclose(delta_function(np.array([1.5, 0.32]), np.array([0.4, 0.1])), np.array([1.1, 0.22]))\n",
    "assert np.allclose(delta_function(np.array([1.5, 0.32, -2.3]), np.array([0.4, 0.1, .0])), np.array([1.1, 0.22, -2.3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden delta tests\n",
    "dummy_mlp = SingleOutputMLP([2, 2])\n",
    "delta_function = dummy_mlp.hidden_backprop_delta\n",
    "\n",
    "assert np.allclose(delta_function(np.array([0.0, 0.0]), np.array([1.0, 1.0]), np.array([1.0, 1.0])), np.array([0.5, 0.5]))\n",
    "assert np.allclose(delta_function(np.array([100.0, 100.0]), np.array([1.0, 1.0]), np.array([1.0, 1.0])), np.array([0, 0]))\n",
    "assert np.allclose(delta_function(np.array([-100.0, -100.0]), np.array([1.0, 1.0]), np.array([1.0, 1.0])), np.array([0.0, 0.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop gradient tests\n",
    "dummy_mlp = SingleOutputMLP([2, 2])\n",
    "grad_function = dummy_mlp.backprop_gradients\n",
    "\n",
    "\n",
    "assert np.allclose(grad_function(np.array([[1., 0.]]), np.array([[1., 1.]])), np.array([[1., 1.], [0., 0.]]))\n",
    "assert np.allclose(grad_function(np.array([[1., 0.], [1., 1.]]), np.array([[1., 1.], [1., 1.]])), np.array([[1., 1.], [0.5, 0.5]]))\n",
    "assert np.allclose(grad_function(np.array([[1., 0.], [1., 1.], [1., 1.], [1., 1.]]), np.array([[1., 1.], [1., 1.], [1., 1.], [1., 1.]])), np.array([[1., 1.], [0.75, 0.75]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We'll apply the training process now. We already implemented the train method in the MLP, so all you need to do is run the code with the data from the first problem. We'll use the same problem as before. Run the following code to see the evolution of the classification on this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source_data.npy', 'rb') as fin:\n",
    "    X = np.load(fin)\n",
    "    \n",
    "with open('label_data.npy', 'rb') as fin:\n",
    "    y = np.load(fin)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BinaryClassMLP = SingleOutputMLP([2, 8, 8, 8], activation=Sigmoid)\n",
    "iterations = 10000\n",
    "\n",
    "p = BinaryClassMLP.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=p.round().flatten(), cmap=plt.cm.Paired, s=20)\n",
    "plt.show()\n",
    "\n",
    "loss_i = 0\n",
    "loss = np.zeros(iterations)\n",
    "for i in range(0, iterations, 200):\n",
    "    BinaryClassMLP.train(X, y, 200)\n",
    "    p = BinaryClassMLP.predict(X)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=p.round().flatten(), cmap=plt.cm.Paired, s=20)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple implementation, without any \"smart\" techniques to stabilize the network. You can try to vary the network architecture, learning rates and other festures, such as the activation function, and find out that it completely misbehaves.\n",
    "\n",
    "Typically, small networks do not work that well, but this is an example for you to have a starting point and explore.\n",
    "\n",
    "Additionally, explore also layer architecture in tensorflow's playground [here](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.91050&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

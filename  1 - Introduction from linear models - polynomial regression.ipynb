{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 - Introduction from linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem and linear models\n",
    "\n",
    "In this notebook we will review the concepts of regression and linear models, and take advantage of this to learn the first steps of gradient descent and training.\n",
    "\n",
    "For this, we'll try to model a polynomial distribution of one single variable, by taking a number of samples from it. We start by generating these samples and showing you the true distribution - see the `polynomial` function.\n",
    "\n",
    "NOTE: The seed is what will allows us reproducibility. Do not change it so that you can compare the outputs during this event. Change it afterwards to see that the method works even with different random samples for the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "seed = 42\n",
    "num_samples = 10\n",
    "\n",
    "\n",
    "# Polynomial distribution to model\n",
    "def polynomial(x):\n",
    "    return x - 0.5 * (x ** 2) + .05 * (x ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by generating a few samples, which will be our observations for the following tasks. We add some gaussian noise so they don't fall exactly on top of the distribution but represent it's shape. Run the following cell and look at the shape of the function and its samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Polynomial distribution \n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = polynomial(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Samples \n",
    "x_samples = np.linspace(0, 10, 10) + np.random.normal(0, 1, num_samples)\n",
    "y_samples = polynomial(x_samples) + np.random.normal(0, 1, num_samples)\n",
    "plt.scatter(x_samples, y_samples, c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our linear model\n",
    "\n",
    "Since we know exactly our model, let's start by trying to obtain an exact solution. We know the number of features we have - 3 - and what they are - $x$, $x^2$ and $x^3$, so our model is defined by:\n",
    "\n",
    "$$a(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "We can also write it in the following manner\n",
    "\n",
    "$$a(x) = w_0 + w_1 x_0 + w_2 x_1 + w_3 x_2$$\n",
    "\n",
    "abstracting away the non-linear characteristic of the features, and you can see that this is a linear model in terms of its parameters, this is, our model is just a linear combination of some input that produces some output.\n",
    "\n",
    "If we put together all of our samples (x, y), we get:\n",
    "\n",
    "$$ \n",
    "x^T = [\\ \\ 0.497,\\ \\ \\ 0.973, \\  ... ,\\  10.543] \\\\\n",
    "y^T = [-0.084,\\ \\ 0.080, \\  ... ,\\   12.145] \\\\\n",
    "$$ \n",
    "\n",
    "But for model $a(x)$ we want to define all features  $x_0$, $x_1$ and $x_2$, building a matrix X that is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \\begin{bmatrix} \n",
    "x_0^0 & x_1^0 & x_2^0 \\\\\n",
    "x_0^1 & x_1^1 & x_2^1 \\\\\n",
    "... & ... & ...\\\\\n",
    "x_0^{n-1} & x_1^{n-1} & x_2^{n-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Implemente a function below that generates this matrix, expanding your original features $x_0^0$, $x_0^1$, ..., $x_0^{n-1}$ into the matrix X. Add an additional collumn of ones, which will represent our bias factors in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_polynomial(samples, num_features):\n",
    "    \"\"\"\n",
    "        Function to expand a vector of samples into polynomial features\n",
    "        \n",
    "        samples - vector of N samples x\n",
    "        num_features - desired degree of expansion f\n",
    "        \n",
    "        The output should be a matrix of size N x f\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_samples = np.array([0.0, 1.0, 2.0])\n",
    "\n",
    "dummy_expanded_samples = np.array(\n",
    "    [\n",
    "        [0.0, 0.0, 0.0, 1.0], \n",
    "        [1.0, 1.0, 1.0, 1.0], \n",
    "        [2.0, 4.0, 8.0, 1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (expand_polynomial(dummy_samples, 3) == dummy_expanded_samples).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact solution\n",
    "\n",
    "\n",
    "Considering $X$ our sample matrix, $w$ as the weight vector, we can define our model as a function $a(x)$ that is computed by the following expression:\n",
    "\n",
    "$$a(X) = Xw$$\n",
    "\n",
    "Given that we have labels $y$ that correspond to the output of $a(x)$, the direct solution would be to use all samples to find an \"exact\" solution, which can be done by using the inverted matrix:\n",
    "\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "\n",
    "Implement it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_solution(x, y):\n",
    "    \"\"\"\n",
    "        Function to compute exact solution of linear problem y = Xw from samples\n",
    "        x and labels y\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use your function to get to the distribution. Like we already mentioned, we'll use 3 as the number of features, so we'll use the expansion function above to greate our feature vectors and then we'll compute the output weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 3\n",
    "X_features = expand_polynomial(x_samples, num_features)\n",
    "w_exact = compute_exact_solution(X_features, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use both these function and your exact solution to get the weights and plot the following:\n",
    "\n",
    "- the original distribution\n",
    "- the samples\n",
    "- your modeled distribution\n",
    "\n",
    "We'll define an auxiliar function that, given a set of weights, implements our modeled polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_polynomial(x, w):\n",
    "    \"\"\"\n",
    "        Auxiliar function to plot polynomial from samples and weights\n",
    "        \n",
    "        Makes use of expansion function to get features\n",
    "    \"\"\"\n",
    "    X_features = expand_polynomial(x, w.shape[0]-1)\n",
    "    return np.dot(X_features, w)\n",
    "\n",
    "# Polynomial distribution \n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = polynomial(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Samples\n",
    "plt.scatter(x_samples, y_samples, c='r', s=10)\n",
    "\n",
    "# Modeled polynomial distribution \n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = model_polynomial(x, w_exact)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "So how do we know how well our model adapts to the data?\n",
    "\n",
    "In this example visualy you can see that it seem to adpata quite well, but in most cases neither you have the true function available, nor is it easy to visualize your data so well. This is why we need to define some function that measures how well we adapt (or not) to the data. We call these error functions or loss function. \n",
    "\n",
    "We will use the mean squared error (MSE). This function is defined by the followin expression:\n",
    "\n",
    "$$L(X, y, w) = \\frac{1}{l} || y - Xw ||^2$$\n",
    "\n",
    "where $X$ is the matrix with the samples, $w$ your weight vector and $y$ the labels. $l$ is the number of samples in the training set. Implement the MSE computation, in the wrapper below that receives two vectors: one with your model predictions and another with the true sample outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mean_squared_errors(X, w, y_gold):\n",
    "    \"\"\"\n",
    "        Function that compute mean squared errors for weights w for\n",
    "        pairs of examples (X, y) where X is the feature matrix and y \n",
    "        the labels\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_samples\n",
    "features = expand_polynomial(x_samples, w_exact.shape[0]-1)\n",
    "\n",
    "assert(np.isclose(loss_mean_squared_errors(features, w_exact, labels), 0.43499541611543535))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent \n",
    "\n",
    "Now we will see how to get to a similar solution through learning methods. In this section, you will implement gradient descent, an algorithm to iteratively update the weights according to the direction of the error. For this  we will need a loss function that describes the mentioned error, and that we can optimize over. The iterative process will then update the weights according to\n",
    "\n",
    "$$ w_{i+1} = w_i - \\eta \\Delta_w L$$ \n",
    "\n",
    "where $\\Delta_w L$ is the gradient of your loss function with respect to the weights, $w_{i+1}$ is the updated weight and $w_i$ the curretn weight. We thus need to compute the gradient of our loss function to be able to define this update process.\n",
    "\n",
    "We'll use the loss function you defined before - the mean squared errors. Notice that the expression you implemented is already adapted to compute the loss over several samples and return the averaged loss. Use the expression below as the $\\Delta_w L$:\n",
    "\n",
    "$$ \\Delta_w = \\frac{2}{N} X^T (Xw - y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for gradient descent implementation\n",
    "def compute_gradient_mse(X, y, w):\n",
    "    \"\"\"\n",
    "        Compute gradient for mean squared errors loss function with respect to\n",
    "        weights w\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise Exception('Not implemented')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll apply your function and update the weights ietratively, at each iteration moving a step closer to the final weights. Run the code below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definitions \n",
    "np.random.seed(seed)\n",
    "num_iterations = 10000\n",
    "num_features = 3\n",
    "eta = .000001    # learning rate\n",
    "w_gradient = 0.0001 * (0.5 - np.random.rand(num_features + 1))  # initial weights\n",
    "\n",
    "\n",
    "### Run gradient descent on our examples\n",
    "X_features = expand_polynomial(x_samples, num_features)\n",
    "for i in range(num_iterations):\n",
    "    w_gradient -= eta * compute_gradient_mse(X_features, y_samples, w_gradient)\n",
    "\n",
    "    \n",
    "### Plot samples and final distribution \n",
    "plt.scatter(x_samples, y_samples, c='r', s=10)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = model_polynomial(x, w_gradient)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, see the error of this model and compare with the exact solution. Vary the number of iterations to see the errors decreasing even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exact error: {}'.format(loss_mean_squared_errors(X_features, w_exact, y_samples)))\n",
    "print('Gradient error: {}'.format(loss_mean_squared_errors(X_features, w, y_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular problem, we mean to show you that the gradient descent method works well in converging to a solution that's close to an exact one. However, this is obviously not the usecase for the learning methods we mean to teach you. \n",
    "\n",
    "Additionally, if you do not pick good initial weights and a good learning rate you won't converge so well to a good solution (try varying those parameters and see for yourself). So it seems too much work for such a simple model. In reality, simply applying the exact solution here is perfectly valid, this was just a simple way for you to get started with the concepts of:\n",
    "\n",
    "- features;\n",
    "- weights;\n",
    "- loss; and\n",
    "- gradient descent.\n",
    "\n",
    "However, these methods are very powerfull, as you will see in the following notebooks. They will be particular usefull for examples when our data is not necessarily defined by a linear model or when the number of features/weights is very big. There are also improvements to the gradient descent algorithm that we're not covering here, with some small differences that stabilize the learning rate and improve convergence, but we will leave those for another time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
